# 微博舆情分析全栈toolkit（保姆级教程，零使用门槛，爬虫+NLP+可视化，一个工具箱全部搞定）
python 版本：Python 3.10.2

注意版本不同可能导致运行时bug

前置准备：将本项目使用git克隆到本地或者直接下载代码

安装依赖： 在Pycharm Terminal（或其他命令行工具）中运行 
```
pip install -r requirements.txt
``` 
也可手动逐个安装

使用时请先关闭VPN，否则可能会使部分模块报错
## 微博博文多字段高速爬虫（可爬IP属地，速度百万条/日）+ 数据清洗脚本
爬虫相关代码全部在crawler.py中，爬虫部分主体代码并非本人所写，API做的很烂，所以使用时需要手动调整输入，但是数据获取速度非常好
，支持以小时为时间粒度爬取，能有效避免获取到的数据都挤在某一时间段内。
支持多线程，可以获取微博**博文**的用户名、发布时间、IP属地、微博正文、转发数、评论数、点赞数等字段，
对于普通的舆情分析任务已经够用了，实测比github上大多数开源爬虫都要更快，并且性能稳定。

下面详细讲解使用方法：
### 获取代理IP
由于微博存在反爬机制，在固定时间内大量访问会导致封号，我们需要使用代理IP来爬取大量数据

这里以熊猫代理为例（不是广告，也可以用自己喜欢的代理IP网站，都一样）

进入网站https://www.xiongmaodaili.com/ ，进行代理IP的购买，三块钱一千个IP，比较合算
![img.png](images/img.png)

在购买完成后，进入个人主页找到订单，然后点击生成API：
![img_1.png](images/img_1.png)

提取数量越多，越利于爬取大量数据，但为了避免浪费，我们一般设置为线程数的偶数倍，比如线程数设置为20，那么提取数量可以设置为40
（后面讲解），
注意生成格式选择txt，然后把这个API链接复制下来一会使用
![img_3.png](images/img_3.png)


### 获取微博cookie（每次爬数据之前重新做一下这个工作）
首先你需要在自己的电脑上安装一个chrome浏览器，然后进入https://weibo.com/ ，登录自己的微博账号。注意不要有任何后缀，只在导航栏中保留weibo.com

![img_4.png](images/img_4.png)

随后按F12或点击浏览器右上角三个点，找到更多工具>开发者工具，选择Network：
![img_5.png](images/img_5.png)

然后点击浏览器左上角刷新页面，刷新时一定要保证导航栏中只有weibo.com而不含有其他后缀：

![img_6.png](images/img_6.png)

此时刚才的页面出现weibo.com，单击：
![img_7.png](images/img_7.png)

在Request Headers中找到Cookie，把这整个一坨都复制下来，粘贴到cookie.txt文件中，这个文件要和
crawler.py在一个文件夹下，注意往里复制之前先把里面之前的内容删一下:
![img_8.png](images/img_8.png)

### 微调代码
#### 修改爬取关键词
找到search类开头处的word_list变量，设置单个想要爬取的关键词，这里以鼠头鸭脖为例：
![img_9.png](images/img_9.png)
#### 修改代理IP的获取API
找到search类开头处的aip_list （忽略这个变量命名错误hhh），把之前复制下来的API链接作为一个字符串整个粘贴进去， ：

![img_10.png](images/img_10.png)

#### 修改年份
找到search类的jx_text方法（该死的拼音命名...），将此处的2023改为获取数据相应的年份，
这里其实很不方便，因为如果要跨年获取的话就得分多次爬：
![img_11.png](images/img_11.png)

#### 修改文件夹名称
这个需要改几个地方，但几个地方要保持完全一致

首先是search类的split_time方法，将下面的鼠头鸭脖改成喜欢的的命名：
![img_12.png](images/img_12.png)

再将代码末尾的几个”鼠头鸭脖“都改成一样的命名，代码运行完成后，获取的数据将以csv文件的形式存放在这个文件夹下（没有的话会自动创建）
![img_13.png](images/img_13.png)

#### 修改爬取时间粒度
这个所谓的时间粒度其实就是爬取的单位时间（以小时为最小粒度），比如设置成4，就是每4小时的数据存放在一个csv文件中
：
修改split_time方法，把那两个4设置成想要的时间粒度即可：
![img_14.png](images/img_14.png)

#### 设置线程数量
从search类的start方法中调整一般代理IP的数量最好设置成线程数量的偶数倍，代理IP越多，能获取的数据上限就越多
一般可以尝试20线程数，代理IP数设置为100，如果IP数目不够（进入死循环并且不再获取数据），那就再加

下面的flag代表线程数：

![img_15.png](images/img_15.png)


#### 设置爬取的时间范围：
把下面三处时间改为想要获取数据的时间范围，注意第一处的时间格式和下面两处不同，即不用补0。时间格式必须严格按模板来，不能随意丢弃空格：

![img_16.png](images/img_16.png)

做完上述工作后，运行代码即可获取数据，程序将自动在当前文件夹下建立一个存储数据的文件夹，数据以csv文件的形式存储

## 句嵌入+情感分类模型（基于Ernie大模型），一键使用
情感分类模型权重链接：https://drive.google.com/file/d/1kXjfj1zQKs-R9-ksZp8tEGuokQ2RMfn6/view?usp=drive_link
## 舆情可视化（舆情数量走势，三维可视化散点图，地图）
## 情感六分类数据集，可供在其他模型上微调，可合并为正负中三类情感
